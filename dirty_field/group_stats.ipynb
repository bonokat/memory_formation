{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats as stats\n",
    "import os\n",
    "os.environ['MNE_3D_OPTION_ANTIALIAS']='false' # to make visualization work\n",
    "import imageio.v3 as iio\n",
    "import pickle\n",
    "\n",
    "import mne\n",
    "from mne.epochs import equalize_epoch_counts\n",
    "from mne.stats import (spatio_temporal_cluster_1samp_test,\n",
    "                       summarize_clusters_stc)\n",
    "from mne.minimum_norm import apply_inverse, read_inverse_operator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects_dir = 'D:\\\\Ekaterina_Voevodina\\\\memory_formation\\\\data\\\\subjects'\n",
    "mris_dir = 'D:\\\\Ekaterina_Voevodina\\\\memory_formation\\\\data\\\\mri'\n",
    "src_fname = os.path.join(mris_dir, 'fsaverage', 'bem', 'fsaverage-ico-5-src.fif')\n",
    "cond1 = 'hits'\n",
    "cond2 = 'miss'\n",
    "fband = 'theta'\n",
    "stcs_dir = os.path.join('D:\\\\Ekaterina_Voevodina\\\\memory_formation\\\\data\\\\group\\\\fsaverage_stc', fband)\n",
    "stc_hits_all, stc_miss_all = [], []\n",
    "\n",
    "for subject_name in os.listdir(subjects_dir):\n",
    "    stcs_hits_path = os.path.join(stcs_dir, f'fsaverage_stc_{subject_name}_{cond1}_{fband}-stc.h5')\n",
    "    stcs_miss_path = os.path.join(stcs_dir, f'fsaverage_stc_{subject_name}_{cond2}_{fband}-stc.h5')\n",
    "    stc_hits = mne.read_source_estimate(stcs_hits_path)\n",
    "    stc_miss = mne.read_source_estimate(stcs_miss_path)\n",
    "    stc_hits_all.append(stc_hits.data)\n",
    "    stc_miss_all.append(stc_miss.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stc_hits_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<SourceEstimate | 20484 vertices, subject : fsaverage, tmin : -1500.0 (ms), tmax : 3995.0 (ms), tstep : 5.0 (ms), data shape : (20484, 1100), ~172.1 MB>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stc_hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20484, 1100)\n",
      "(20484, 1100)\n",
      "(20484, 1100)\n",
      "(20484, 1100)\n",
      "(20484, 1100)\n",
      "(20484, 1100)\n",
      "(20484, 1100)\n",
      "(20484, 1100)\n",
      "(20484, 1100)\n",
      "(20484, 1100)\n",
      "(20484, 1100)\n",
      "(20484, 1100)\n",
      "(20484, 1100)\n",
      "(20484, 1100)\n",
      "(20484, 1100)\n",
      "(20484, 1100)\n",
      "(20484, 1100)\n",
      "(20484, 1100)\n",
      "(20484, 1100)\n"
     ]
    }
   ],
   "source": [
    "for stc in stc_hits_all:\n",
    "    print(stc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.stack([np.stack(stc_hits_all, -1),\n",
    "np.stack(stc_miss_all, -1)], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Reading a source space...\n",
      "    [done]\n",
      "    Reading a source space...\n",
      "    [done]\n",
      "    2 source spaces read\n",
      "-- number of adjacent vertices : 20484\n"
     ]
    }
   ],
   "source": [
    "src = mne.read_source_spaces(src_fname)\n",
    "adjacency = mne.spatial_src_adjacency(src)\n",
    "fsave_vertices = [s['vertno'] for s in src]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20484, 1100, 19, 2)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.abs(X)  # only magnitude\n",
    "X = X[:, :, :, 0] - X[:, :, :, 1]  # make paired contrast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.transpose(X, [2, 1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering.\n",
      "stat_fun(H1): min=-8.541796 max=5.935616\n",
      "Running initial clustering â€¦\n",
      "Found 2558 clusters\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a49376e8a2f4947949b812eafa3b34a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | Permuting : 0/1023 [00:00<?,       ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p_threshold = 0.001\n",
    "df = len(os.listdir(subjects_dir)) - 1  # degrees of freedom for the test\n",
    "t_threshold = stats.distributions.t.ppf(1 - p_threshold / 2, df=df)\n",
    "\n",
    "# Now let's actually do the clustering. This can take a long time...\n",
    "print('Clustering.')\n",
    "T_obs, clusters, cluster_p_values, H0 = clu = \\\n",
    "    spatio_temporal_cluster_1samp_test(X, adjacency=adjacency, n_jobs=None,\n",
    "                                       threshold=t_threshold, buffer_size=None,\n",
    "                                       verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(cluster_p_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVING THE DATA\n",
    "# save np.arrays\n",
    "path_to_save = 'D:\\\\Ekaterina_Voevodina\\\\memory_formation\\\\data\\\\group\\\\statistics'\n",
    "X_path_to_save = os.path.join(path_to_save, f'X_{cond1}_VS_{cond2}_{fband}')\n",
    "T_obs_path_to_save = os.path.join(path_to_save, f'T_obs_{cond1}_VS_{cond2}_{fband}')\n",
    "cluster_p_values_path_to_save = os.path.join(path_to_save, f'cluster_p_values_{cond1}_VS_{cond2}_{fband}')\n",
    "H0_path_to_save = os.path.join(path_to_save, f'H0_{cond1}_VS_{cond2}_{fband}')\n",
    "\n",
    "np.save(X_path_to_save, X)\n",
    "np.save(T_obs_path_to_save, T_obs)\n",
    "np.save(cluster_p_values_path_to_save, cluster_p_values)\n",
    "np.save(H0_path_to_save, H0)\n",
    "\n",
    "# save lists\n",
    "# save stc_hits_all\n",
    "with open(os.path.join(path_to_save, f'stc_hits_all_{cond1}_VS_{cond2}_{fband}'), \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(stc_hits_all, fp)\n",
    "    \n",
    "# save stc_miss_all\n",
    "with open(os.path.join(path_to_save, f'stc_miss_all_{cond1}_VS_{cond2}_{fband}'), \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(stc_miss_all, fp)\n",
    "\n",
    "# save clusters\n",
    "with open(os.path.join(path_to_save, f'clusters_{cond1}_VS_{cond2}_{fband}'), \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(clusters, fp)\n",
    "\n",
    "# save clu\n",
    "with open(os.path.join(path_to_save, f'clu_{cond1}_VS_{cond2}_{fband}'), \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(clu, fp)\n",
    "\n",
    "# save fsave_vertices\n",
    "with open(os.path.join(path_to_save, 'fsave_vertices'), \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(fsave_vertices, fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOADING saved files\n",
    "X_file = np.load(X_path_to_save + '.npy')\n",
    "T_obs_file = np.load(T_obs_path_to_save + '.npy')\n",
    "cluster_p_values_file = np.load(cluster_p_values_path_to_save + '.npy')\n",
    "H0_file = np.load(H0_path_to_save + '.npy')\n",
    "\n",
    "# load stc_hits_all\n",
    "with open(os.path.join(path_to_save, f'stc_hits_all_{cond1}_VS_{cond2}_{fband}'), \"rb\") as fp:   # Unpickling\n",
    "    stc_hits_all_file = pickle.load(fp)\n",
    "\n",
    "# load stc_miss_all\n",
    "with open(os.path.join(path_to_save, f'stc_miss_all_{cond1}_VS_{cond2}_{fband}'), \"rb\") as fp:   # Unpickling\n",
    "    stc_miss_all_file = pickle.load(fp)\n",
    "\n",
    "# load clusters\n",
    "with open(os.path.join(path_to_save, f'clusters_{cond1}_VS_{cond2}_{fband}'), \"rb\") as fp:   # Unpickling\n",
    "    clusters_file = pickle.load(fp)\n",
    "\n",
    "# load clu\n",
    "with open(os.path.join(path_to_save, f'clu_{cond1}_VS_{cond2}_{fband}'), \"rb\") as fp:   # Unpickling\n",
    "    clu_file = pickle.load(fp)\n",
    "\n",
    "# load fsave_vertices\n",
    "with open(os.path.join(path_to_save, 'fsave_vertices'), \"rb\") as fp:   #Pickling\n",
    "    fsave_vertices_file = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cheching for equality: information before and after saving\n",
    "#! DO NOT INCLUDE IN FINAL SCRIPT\n",
    "\n",
    "for i in range(len(clusters_file)):\n",
    "    if np.array_equal(clusters_file[i], clusters[i]):\n",
    "        continue\n",
    "    else:\n",
    "        print(f'{i} NOT ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the clusters that are statistically significant at p < 0.05\n",
    "good_clusters_idx = np.where(cluster_p_values < 0.05)[0]\n",
    "good_clusters = [clusters[idx] for idx in good_clusters_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([244, 253, 436, 521, 681, 836], dtype=int64)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_clusters_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(fsave_vertices[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # trying to make visualization work\n",
    "#! DOES NOT HELP :(\n",
    "\n",
    "# mne.viz.set_3d_options(depth_peeling=False, antialias=True, multi_samples=1)\n",
    "# mne.viz.set_3d_options(antialias=False, depth_peeling=True, smooth_shading=False, multi_samples=1)\n",
    "\n",
    "# # OR\n",
    "\n",
    "# os.environ['MNE_3D_OPTION_ANTIALIAS']='false'\n",
    "# os.environ['MNE_3D_OPTION_DEPTH_PEELING']='false'\n",
    "# os.environ['MNE_3D_OPTION_SMOOTH_SHADING']='false'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "stc_all_cluster_vis = summarize_clusters_stc(clu_file, #or clu\n",
    "                                             vertices=fsave_vertices_file, #or fsave_vertices\n",
    "                                             subject='fsaverage')\n",
    "\n",
    "# blue blobs are for condition A < condition B, red for A > B\n",
    "brain = stc_all_cluster_vis.plot(\n",
    "    hemi='both', views='lateral', subjects_dir=mris_dir,\n",
    "    time_label='temporal extent (ms)', size=(800, 800),\n",
    "    smoothing_steps=5, clim=dict(kind='value', pos_lims=[0, 1, 40]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mne.source_estimate.SourceEstimate"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(stc_all_cluster_vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20484"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stc_all_cluster_vis.data > 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aa758a63b168369e3aec664aef7a38b0d5054e3c5200c364b30f3533ef2a8043"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
